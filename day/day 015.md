# day 15



## tensofow로 bmi 판정

```python
#tensofow로 bmi 판정
import pandas as pd
import numpy as np
import tensorflow as tf

df = pd.read_csv("C:/Users/student/dataset/bmi.csv")

# 키와 몸무게 정규화
df["height"] = df["height"] / 200
df["weight"] = df["weight"] / 100

# label 컬럼 변환 - thin[1,0,0]/normal[0,1,0]/fat[0,0,1]
bclass = {"thin":[1,0,0], "normal":[0,1,0], "fat":[0,0,1]}
df["label_pat"] = df["label"].apply(lambda x:np.array(bclass[x]))

# 학습데이터와 테스트 데이터 분류
test_df = df[15000:20000]
test_pat = test_df[["weight", "height"]]
test_ans = list(test_df["label_pat"])

X = tf.placeholder(tf.float32, [None, 2])  # 키, 몸무게 데이터 담을 placeholder 선언
Y = tf.placeholder(tf.float32, [None, 3])  # 정답 레이블 데이터 담을 placeholder 선언

W = tf.Variable(tf.zeros([2,3]))
b = tf.Variable(tf.zeros([3]))

y = tf.nn.softmax(tf.matmul(X, W)+b)  # 소프트맥스 회귀 정의
cross_entropy = -tf.reduce_sum(Y * tf.log(y))   # 오차함수 - 교차 엔트로피
train = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)   #경사하강법으로 학습

# 예측값, 정답률 계산
predict = tf.equal(tf.argmax(y,1), tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))

sess = tf.Session()
sess.run(tf.global_variables_initializer()) 
for step in range(10501):
    i = (step*100) % 14000
    rows = df[i+1:i+1+100]
    x_pat = rows[["weight", "height"]]
    y_ans = list(rows["label_pat"])
    sess.run(train, feed_dict={X:x_pat, Y:y_ans})
    if step%500 == 0:
        cre = sess.run(cross_entropy, feed_dict={X:x_pat, Y:y_ans})
        acc = sess.run(accuracy, feed_dict={X:test_pat, Y:test_ans})
        print("Epoch=", step, "오치=", cre, "정확률(평균)=", acc)
        

```



## keras + tensofow로 bmi 판정

```python
#keras + tensofow로 bmi 판정
import pandas as pd
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.callbacks import EarlyStopping

df = pd.read_csv("C:/Users/student/dataset/bmi.csv")

#키와 몸무게 정규화
df["height"] = df["height"] / 200
df["weight"] = df["weight"] / 100

#label 컬럼 변환 - thin[1, 0, 0]/normal[0, 1, 0]/fat [0, 0, 1]
bclass = {"thin": [1, 0, 0] , "normal":[0, 1, 0], "fat": [0, 0, 1]}
df["label_pat"] = df["label"].apply(lambda x: np.array(bclass[x]))

#학습데이터와 테스트 데이터 분류
test_df = df[15000:20000]
test_pat= test_df[["weight", "height"]]
test_ans = list(test_df["label_pat"])

X = tf.placeholder(tf.float32, [None, 2]) #키, 몸부게 데이터 담을 placeholder  선언
Y = tf.placeholder(tf.float32, [None, 3])   #정답 레이블 데이터 담을 placeholder  선언

W = tf.Variable(tf.zeros([2, 3])) 
b = tf.Variable(tf.zeros([3])) 

y = tf.nn.softmax(tf.matmul(X, W) + b)  #소프트맥스 회귀 정의
cross_entropy = -tf.reduce_sum(Y * tf.log(y))  #오차함수 - 교차 엔트로피
train= tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)  #경사하강법으로 학습
 
#예측값, 정답률 계산
predict = tf.equal(tf.argmax(y, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))

sess = tf.Session()
sess.run(tf.global_variables_initializer()) 
for step in range(3501):
    i = (step*100) % 14000
    rows = df[i+1:i+1+100]
    x_pat = rows[["weight", "height"]]
    y_ans =  list(rows["label_pat"])
    sess.run(train, feed_dict={X: x_pat  , Y: y_ans })
    if step%500  == 0 :
        cre = sess.run(cross_entropy, feed_dict={X: x_pat  , Y: y_ans })
        acc = sess.run(accuracy , feed_dict={X: test_pat  , Y: test_ans })
        print("Epoch=", step, "오차=", cre, "정확률(평균)=", acc)
```



## #폐암수술 환자 예측 분석 모델

```python
#폐암수술 환자 예측 분석 모델
# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.
from keras.models import Sequential
from keras.layers import Dense


# 필요한 라이브러리를 불러옵니다.
import numpy
import tensorflow as tf

# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.
seed = 0
numpy.random.seed(seed)
tf.set_random_seed(seed)

# 준비된 수술 환자 데이터를 불러들입니다.
Data_set = numpy.loadtxt("C:/Users/student/dataset/ThoraricSurgery.csv",delimiter=",")

# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장합니다.
X = Data_set[:,0:17]
Y = Data_set[:,17]

# 딥러닝 구조를 결정합니다(모델을 설정하고 실행).
model = Sequential()   
# 첫 번째 은닉층에 input_dim을 적어 줌으로써 첫 번째 Dense가 은닉층 + 입력층의 역할을 겸합니다.
# 데이터에서 17개의 값을 받아 은닉층의 30개 노드로 보낸다 
model.add(Dense(30, input_dim=17, activation='relu'))  #activation : 출력층으로 전달할 때 사용할 활성화 함수
model.add(Dense(1, activation='sigmoid'))  #출력층의 노드 수는 1개, 최종 출력 값에 사용될 활성화 함수

# 딥러닝을 실행합니다. (오차 함수 :  평균 제곱 오차 함수 사용)
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=30, batch_size=10)

# 결과를 출력합니다.
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))

```



## 이항 분류 분석 딥러닝으로 학습

```pyhton
#이항 분류 분석 딥러닝으로 학습
from keras.models import Sequential
from keras.layers import Dense
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)                   # seed 값 생성
tf.set_random_seed(seed)

dataset = numpy.loadtxt("C:/Users/student/dataset/pima-indians-diabets.csv",delimiter=",")               # 데이터 로드
X1 = dataset[:,0:8]
Y1 = dataset[:,8]

model = Sequential()                                                # 모델의 설정
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',   optimizer='adam',      metrics=['accuracy'])   # 모델 컴파일
model.fit(X1, Y1, epochs=200, batch_size=10)                                # 모델 실행
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))                # 결과 출력
```



## 다중  분류 분석 딥러닝으로 학습

```python
#다중  분류 분석 딥러닝으로 학습
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv('C:/Users/student/dataset/iris.csv', names = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"])
print(df.head())

sns.pairplot(df, hue='species')  #속성별 연관성 파악
plt.show()

dataset = df.values
X = dataset[:,0:4].astype(float)
Y_obj = dataset[:,4]

from sklearn.preprocessing import LabelEncoder     
e = LabelEncoder()     # array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])가 array([1,2,3])로 변환
e.fit(Y_obj)
Y = e.transform(Y_obj)

from keras.utils import np_utils
# array([1,2,3])가 다시 array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])로 원-핫 인코딩(one-hot-encoding) 변환
Y_encoded = np_utils.to_categorical(Y)

from keras.models import Sequential
from keras.layers.core import Dense
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed) # seed 값 설정
tf.set_random_seed(seed)

model = Sequential() # 모델의 설정
model.add(Dense(16, input_dim=4, activation='relu'))
#최종 출력 값이 3개 중 하나여야 하므로 출력층에 해당하는 Dense의 노드 수를 3으로 설정
model.add(Dense(3, activation='softmax'))

# 모델 컴파일(다중 분류에 적절한 오차 함수인 categorical_crossentropy를 사용, 최적화 함수로 adam 사용)
model.compile(loss='categorical_crossentropy',    optimizer='adam',    metrics=['accuracy'])

# 모델 실행(한 번에 입력되는 값은 1개, 전체 샘플이 50회 반복될 때까지 실험을 진행
model.fit(X, Y_encoded, epochs=50, batch_size=1)   

print("\n Accuracy: %.4f" % (model.evaluate(X, Y_encoded)[1]))   # 결과 출력
```



## 초음파 광물 분석

```python
#초음파 광물 분석
from keras.models import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)        # seed 값 설정
tf.set_random_seed(seed)

df = pd.read_csv('C:/Users/student/dataset/sonar.csv', header=None)           # 데이터 입력
dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]
#print(Y_obj.unique())

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)            # 문자열 변환

model = Sequential()              # 모델 설정
model.add(Dense(24, input_dim=60, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='mean_squared_error',   optimizer='adam',    metrics=['accuracy'])  # 모델 컴파일
model.fit(X, Y, epochs=200, batch_size=5)                      # 모델 실행
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))     # 결과 출력
```



``` python
#학습 데이터와 테스트 데이터가 중복되면 과적합이 발생합니다.
#데이터를 7:3의 비율로 랜덤하게 학습 데이터와 테스트 데이터로 분리해서 모델을 학습시키고 정확도를 측정합니다.
from keras.models import Sequential, load_model
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)        # seed 값 설정
tf.set_random_seed(seed)

df = pd.read_csv('C:/Users/student/dataset/sonar.csv', header=None)           # 데이터 입력
dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)            # 문자열 변환

# 학습셋과 테스트셋의 구분
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)

model = Sequential()
model.add(Dense(24, input_dim=60, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
#학습 데이터로 학습
model.fit(X_train, Y_train, epochs=130, batch_size=5)
model.save('C:/Users/student/output/my_model.h5')   # 학습을 통해 생성된 모델을 컴퓨터에 저장

del model   # 테스트를 위해 메모리 내의 모델을 삭제
model = load_model('C:/Users/student/output/my_model.h5')   # 모델을 새로 불러옴

print("\n Test Accuracy: %.4f" % (model.evaluate(X_test, Y_test)[1]))   # 불러온 모델로 테스트 실행
```



## 데이터 샘플이 적은 경우 : k-겹 교차 검증

```python
#데이터 샘플이 적은 경우 : k-겹 교차 검증
#10개의 파일로 쪼개 테스트하는?10-fold cross validation을 실시하도록?n_fold의 값을 10으로 설정한 뒤?StratifiedKFold()?함수에 적용했습니다. 그런 다음 모델을 만들고 실행하는 부분을?for?구문으로 묶어?n_fold만큼 반복되게 합니다.
from keras.models import Sequential, load_model
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy
import tensorflow as tf

seed = 0
numpy.random.seed(seed)        # seed 값 설정
tf.set_random_seed(seed)

df = pd.read_csv('C:/Users/student/dataset/sonar.csv', header=None)           # 데이터 입력
dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)            # 문자열 변환

from sklearn.model_selection import StratifiedKFold

n_fold = 10  #10겹 
skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)

accuracy = []  # 빈 accuracy 배열

for train, test in skf.split(X, Y):  # 모델의 설정, 컴파일, 실행
    model = Sequential()
    model.add(Dense(24, input_dim=60, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='mean_squared_error',  optimizer='adam',   metrics=['accuracy'])
    model.fit(X[train], Y[train], epochs=100, batch_size=5)
    k_accuracy = "%.4f" % (model.evaluate(X[test], Y[test])[1])
    accuracy.append(k_accuracy)

# 결과 출력
print("\n %.f fold accuracy:" % n_fold, accuracy)
```



```python
#Epoch마다 모델의 정확도를 기록, 저장 (파일 형식 .hdf5) 
# 모델을 저장하기 위햇 ModelCheckpoint함수를 콜백함수로 사용 - 모니터할 값 지정 (loss, acc  ,val_loss, val_acc ), verbose : 정보 표시 상세도 (0, 1, 2) 지정
# save_best_only은 정확도가 이전 학습보다 개선되었을때만 저장하도록 함
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt
import os

seed = 0
numpy.random.seed(seed)  # seed 값 설정
tf.set_random_seed(seed)

df_pre = pd.read_csv('C:/Users/student/dataset/wine.csv', header=None)
df = df_pre.sample(frac=1)  #rac = 1 지정은 원본 데이터의 100%를 불러오라는 의미
dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

model = Sequential() # 모델 설정(4개의 은닉층을 만들어 각각 30, 12, 8, 1개의 노드를 주었습니다)
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
 
from keras.callbacks import ModelCheckpoint

# 모델 컴파일
model.compile(loss='binary_crossentropy',   optimizer='adam',    metrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR = 'C:/Users/student/output/model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

#테스트 오차는 케라스 내부에서 val_loss, 학습 정확도는 acc, 테스트셋 정확도는 val_acc, 학습셋 오차는 loss로 각각 기록됩니다
# 모델 저장 조건 설정
modelpath="C:/Users/student/output/model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

# 모델 실행 및 저장
model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200,verbose=0, callbacks=[checkpointer])

```



```python
#Epoch마다 모델의 정확도를 기록, 저장 (파일 형식 .hdf5) 
# 모델을 저장하기 위햇 ModelCheckpoint함수를 콜백함수로 사용 - 모니터할 값 지정 (loss, acc  ,val_loss, val_acc ), verbose : 정보 표시 상세도 (0, 1, 2) 지정
# save_best_only은 정확도가 이전 학습보다 개선되었을때만 저장하도록 함
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt
import os

seed = 0
numpy.random.seed(seed)  # seed 값 설정
tf.set_random_seed(seed)

df_pre = pd.read_csv('C:/Users/student/dataset/wine.csv', header=None)
df = df_pre.sample(frac=1)  #rac = 1 지정은 원본 데이터의 100%를 불러오라는 의미
dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

model = Sequential() # 모델 설정(4개의 은닉층을 만들어 각각 30, 12, 8, 1개의 노드를 주었습니다)
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
 
from keras.callbacks import ModelCheckpoint

# 모델 컴파일
model.compile(loss='binary_crossentropy',   optimizer='adam',    metrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR = 'C:/Users/student/output/model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

#테스트 오차는 케라스 내부에서 val_loss, 학습 정확도는 acc, 테스트셋 정확도는 val_acc, 학습셋 오차는 loss로 각각 기록됩니다
# 모델 저장 조건 설정
modelpath="C:/Users/student/output/model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

history = model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)

# y_vloss에 테스트셋(33%)으로 실험 결과의 오차 값을 저장
y_vloss=history.history['val_loss']

# y_acc에 학습셋(67%)으로 측정한 정확도의 값을 저장
y_acc=history.history['accuracy']

# x 값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시
x_len = numpy.arange(len(y_acc))
plt.plot(x_len, y_vloss, "o", c="red", markersize=3)
plt.plot(x_len, y_acc, "o", c="blue", markersize=3)

plt.show()
```



```python
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt

seed = 0
numpy.random.seed(seed)  # seed 값 설정
tf.set_random_seed(seed)

df_pre = pd.read_csv('C:/Users/student/dataset/wine.csv', header=None)
df = df_pre.sample(frac=0.15)  #rac = 1 지정은 원본 데이터의 100%를 불러오라는 의미
dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

model = Sequential() # 모델 설정(4개의 은닉층을 만들어 각각 30, 12, 8, 1개의 노드를 주었습니다)
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy',   optimizer='adam',    metrics=['accuracy'])
# 모델 저장 폴더 설정
MODEL_DIR = 'C:/Users/student/output/model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

#테스트 오차는 케라스 내부에서 val_loss, 학습 정확도는 acc, 테스트셋 정확도는 val_acc, 학습셋 오차는 loss로 각각 기록됩니다
# 모델 저장 조건 설정
modelpath="C:/Users/student/output/model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

from keras.callbacks import EarlyStopping

# 자동 중단 설정(EarlyStopping() 함수에 모니터할 값과 테스트 오차가 좋아지지 않아도 몇 번까지 기다릴지를 정합니다.)
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)

# 모델 실행
model.fit(X, Y, validation_split=0.2, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))
```

